{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d9db55-5b34-48c2-9ac2-6a4620003eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyhdf\n",
    "from pyhdf.SD import SD, SDC\n",
    "from pyhdf.HDF import *\n",
    "from pyhdf.VS import *\n",
    "from datetime import datetime, timezone\n",
    "from scipy import interpolate\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a6b62b-a348-4f9f-9bcf-12bbcd8c659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===============================================\n",
    "def read_2b_cldclass_lidar(cs_file,latbin=None):\n",
    "## ===============================================\n",
    "  f_2b_cldclass_lidar=SD(cs_file, SDC.READ)\n",
    "  sds_obj=f_2b_cldclass_lidar.select('CloudLayerBase')\n",
    "  cs_clb =sds_obj.get()\n",
    "  sds_obj=f_2b_cldclass_lidar.select('CloudLayerTop')\n",
    "  cs_clt =sds_obj.get()\n",
    "\n",
    "  ## geolocation fields\n",
    "  sdc_2bcldclass_lidar=HDF(cs_file, SDC.READ)\n",
    "  vs_2bcldclass_lidar=sdc_2bcldclass_lidar.vstart()\n",
    "  cs_QC = np.squeeze(vs_2bcldclass_lidar.attach('Data_quality')[:])\n",
    "  Latitude = np.squeeze(vs_2bcldclass_lidar.attach('Latitude')[:])\n",
    "  Longitude = np.squeeze(vs_2bcldclass_lidar.attach('Longitude')[:])\n",
    "\n",
    "  ilat = np.squeeze(np.argwhere((Latitude >= latbin[0]) & (Latitude <= latbin[1])))\n",
    "  cs_clb = cs_clb[ilat,:]\n",
    "  cs_clt = cs_clt[ilat,:]\n",
    "  cs_QC = cs_QC[ilat]\n",
    "  Latitude = Latitude[ilat]\n",
    "  Longitude = Longitude[ilat]\n",
    "\n",
    "  return(cs_clb,cs_clt,cs_QC,Latitude,Longitude)\n",
    "\n",
    "## ===============================================\n",
    "def read_cs_ecmwf(aux_file,latbin=None):\n",
    "## ===============================================\n",
    "  f_ecmwf=SD(aux_file, SDC.READ)\n",
    "  sds_obj=f_ecmwf.select('Pressure')\n",
    "  Pressure =sds_obj.get()\n",
    "  sds_obj=f_ecmwf.select('Temperature')\n",
    "  Temperature =sds_obj.get()\n",
    "  sds_obj=f_ecmwf.select('Specific_humidity')\n",
    "  Specific_humidity =sds_obj.get()\n",
    "  #sds_obj=f_ecmwf.select('U_velocity')\n",
    "  #U_velocity =sds_obj.get()\n",
    "  #sds_obj=f_ecmwf.select('V_velocity')\n",
    "  #V_velocity =sds_obj.get()\n",
    "\n",
    "  ## geolocation fields\n",
    "  sdc_ecmwf=HDF(aux_file, SDC.READ)\n",
    "  vs_ecmwf=sdc_ecmwf.vstart()\n",
    "  EC_height = np.squeeze(vs_ecmwf.attach('EC_height')[:])\n",
    "  Profile_time = np.squeeze(vs_ecmwf.attach('Profile_time')[:])\n",
    "  UTC_start = np.squeeze(vs_ecmwf.attach('UTC_start')[:])\n",
    "  #TAI_start = np.squeeze(vs_ecmwf.attach('TAI_start')[:])\n",
    "  Latitude = np.squeeze(vs_ecmwf.attach('Latitude')[:])\n",
    "  Longitude = np.squeeze(vs_ecmwf.attach('Longitude')[:])\n",
    "  DEM_elevation = np.squeeze(vs_ecmwf.attach('DEM_elevation')[:])\n",
    "  Skin_temperature = np.squeeze(vs_ecmwf.attach('Skin_temperature')[:])\n",
    "  Surface_pressure = np.squeeze(vs_ecmwf.attach('Surface_pressure')[:])\n",
    "  Temperature_2m = np.squeeze(vs_ecmwf.attach('Temperature_2m')[:])\n",
    "  #Sea_surface_temperature = np.squeeze(vs_ecmwf.attach('Sea_surface_temperature')[:])\n",
    "  U10_velocity = np.squeeze(vs_ecmwf.attach('U10_velocity')[:])\n",
    "  V10_velocity = np.squeeze(vs_ecmwf.attach('V10_velocity')[:])\n",
    "\n",
    "\n",
    "  UTC_Time = UTC_start + Profile_time\n",
    "  UTC_Time = UTC_Time/60./60.\n",
    "  ilat = np.squeeze(np.argwhere((Latitude >= latbin[0]) & (Latitude <= latbin[1])))\n",
    "  Pressure = Pressure[ilat,:]\n",
    "  Temperature = Temperature[ilat,:]\n",
    "  Specific_humidity = Specific_humidity[ilat,:]\n",
    "  DEM_elevation = DEM_elevation[ilat]\n",
    "  Skin_temperature = Skin_temperature[ilat]\n",
    "  Temperature_2m = Temperature_2m[ilat]\n",
    "  U10_velocity = U10_velocity[ilat]\n",
    "  V10_velocity = V10_velocity[ilat]\n",
    "  UTC_Time = UTC_Time[ilat]\n",
    "\n",
    "\n",
    "  return(Pressure,DEM_elevation,Temperature,Specific_humidity,EC_height,Temperature_2m,U10_velocity,V10_velocity,UTC_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd221f8-9f59-4175-9a7d-a5ceb26cdd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory, prefix):\n",
    "    matching_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(prefix):\n",
    "            matching_files.append(os.path.join(directory, filename))\n",
    "    return matching_files\n",
    "\n",
    "def gather_files(YYYY, DDD, HH, ROOT):\n",
    "    ABI_ = {\n",
    "        \"ROOT_PATH\": None,\n",
    "\n",
    "        \"YYYY\": None,\n",
    "        \"DDD\": None,\n",
    "        \"HH\": None,\n",
    "\n",
    "        \"00\": [],\n",
    "        \"10\": [],\n",
    "        \"15\": [],\n",
    "        \"20\": [],\n",
    "        \"30\": [],\n",
    "        \"40\": [],\n",
    "        \"45\": [],\n",
    "        \"50\": [],\n",
    "\n",
    "        \"L200\": None,\n",
    "        \"L210\": None,\n",
    "        \"L220\": None,\n",
    "        \"L230\": None,\n",
    "        \"L240\": None,\n",
    "        \"L250\": None,\n",
    "\n",
    "        \"everyten\": False,\n",
    "    }\n",
    "\n",
    "    _ABI_PATH_ = ROOT + YYYY + \"/\" + DDD + \"/\" + HH\n",
    "\n",
    "    for filename in os.listdir(_ABI_PATH_):\n",
    "        if ABI_[\"ROOT_PATH\"] == None:\n",
    "            ABI_[\"ROOT_PATH\"] = _ABI_PATH_\n",
    "            ABI_[\"YYYY\"] = filename[27:31]\n",
    "            ABI_[\"DDD\"] = filename[31:34]\n",
    "            ABI_[\"HH\"] = filename[34:36]\n",
    "        MM = filename[36:38]\n",
    "        if MM == \"10\":\n",
    "            ABI_[\"everyten\"] = True\n",
    "        ABI_[f\"{MM}\"].append(filename)\n",
    "    \n",
    "    return ABI_\n",
    "\n",
    "def get_L1B_L2(abipaths, l2path, YYYY, DDD, HH, ROOT):\n",
    "\n",
    "    # ASSERT ALL ABI CHANNELS PRESENT !!!!!!!!!!!!!!!!!!!!\n",
    "    if len(abipaths) != 16:\n",
    "        raise ImportError(\"This hour is bad\")\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "    # LOAD EACH ABI CHANNEL IMAGE\n",
    "    CHANNELS = []\n",
    "    # ROOT = \"/css/geostationary/BackStage/GOES-16-ABI-L1B-FULLD/\"\n",
    "\n",
    "    for file in abipaths:\n",
    "        L1B = np.array(nc.Dataset(ROOT + \"/\" + YYYY + \"/\" + DDD + \"/\" + HH + \"/\" + file, 'r')[\"Rad\"])\n",
    "        CHANNEL = int(file[19:21])\n",
    "        CHANNELS.append((L1B, CHANNEL))\n",
    "\n",
    "    # SORT CHANNELS\n",
    "    CHANNELS.sort(key=lambda x: x[1])\n",
    "    CHANNELS = [C[0] for C in CHANNELS]\n",
    "\n",
    "    T = []\n",
    "    #RESIZE ALL CHANNELS TO SAME SIZE\n",
    "    for C in CHANNELS:\n",
    "        S = C.shape[0] // 5424\n",
    "        if S == 1:\n",
    "            C = np.repeat(C, 2, axis=0)\n",
    "            C = np.repeat(C, 2, axis=1)\n",
    "        if S == 4:\n",
    "            C = C[::2, ::2]\n",
    "        T.append(C)\n",
    "\n",
    "    CHANNELS = T\n",
    "\n",
    "    # STACK ABI CHANNELS INTO SINGLE IMAGE\n",
    "    ABI = np.stack(CHANNELS, axis=2)\n",
    "\n",
    "    return ABI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244d21f0-147c-4421-962f-ec4e0198ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOUND_SIZE = 2000\n",
    "LENGTH = 10848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfb035a-446d-4b72-8b12-2df854efb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN ABI_EAST in order to find the latitude and longitude of each file\n",
    "f = nc.Dataset(\"../ABI_EAST_GEO_TOPO_LOMSK.nc\")\n",
    "abiLong = np.array(f['Longitude'])\n",
    "abiLat = np.array(f['Latitude'])\n",
    "\n",
    "# Get the latitude and longitude boundaries within the bounded region\n",
    "\n",
    "abiLongB = abiLong[BOUND_SIZE:LENGTH-BOUND_SIZE, BOUND_SIZE:LENGTH-BOUND_SIZE]\n",
    "abiLatB = abiLat[BOUND_SIZE:LENGTH-BOUND_SIZE, BOUND_SIZE:LENGTH-BOUND_SIZE]\n",
    "abiLongB[abiLongB == -999] = 10\n",
    "abiLatB[abiLatB == -999] = 10\n",
    "longMin = abiLongB.min()\n",
    "longMax = abiLongB.max()\n",
    "latMin = abiLatB.min()\n",
    "latMax = abiLatB.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adf036a-036a-4267-a246-9f8a5a1291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if they loaded correctly\n",
    "# abiLongG = abiLong.copy()\n",
    "# abiLatG = abiLat.copy()\n",
    "\n",
    "# abiLongG = abiLongG.flatten()\n",
    "# abiLatG = abiLatG.flatten()\n",
    "\n",
    "# mask = (abiLongG != -999) & (abiLatG != -999)\n",
    "# abiLongG = abiLongG[mask]\n",
    "# abiLatG = abiLatG[mask]\n",
    "\n",
    "# print(abiLongG.shape)\n",
    "\n",
    "# abiLongG = abiLongG[::1000]\n",
    "# abiLatG = abiLatG[::1000]\n",
    "\n",
    "# plt.scatter(abiLongG, abiLatG, c='blue', marker='o')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# im0 = axes[0].imshow(abiLongG)\n",
    "# im1 = axes[1].imshow(abiLatG)\n",
    "# # axes[0].axis('off')\n",
    "# # axes[1].axis('off')\n",
    "# axes[0].set_title(\"Longitude\")\n",
    "# axes[1].set_title(\"Latitude\")\n",
    "# plt.colorbar(im0, ax=axes[0])\n",
    "# plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# TEST FOOTPRINT SIZE\n",
    "# for i in np.arange(5000,5100):\n",
    "#   coords_1 = (abiLat[i,5000],abiLong[i,5000])\n",
    "#   coords_2 = (abiLat[i+1,5000],abiLong[i+1,5000])\n",
    " \n",
    "#   dis = geodesic(coords_1,coords_2).km\n",
    "#   print(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccba55a-6116-4906-ab58-0886d42ed462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store already parsed files for more efficiency\n",
    "\n",
    "GATHERED_ABI_FILES = {}\n",
    "COLLECTED_ABI_DATA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c95b76b-59f6-43d4-950a-0c4ba4491023",
   "metadata": {},
   "outputs": [],
   "source": [
    "latSlice = abiLat[:, 5424]\n",
    "latSlice = latSlice[18:-18]\n",
    "longSlice = abiLong[5424, :]\n",
    "longSlice = longSlice[18:-18]\n",
    "latSlice = latSlice[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51bb2230-d9a8-4fe8-9ea8-ecf6a51bde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpArray(a):\n",
    "    N = len(a)\n",
    "    M = len(a[0])\n",
    "    a_interp = np.zeros((N, 40))\n",
    "\n",
    "    for i in range(N):\n",
    "        f = interpolate.interp1d(np.linspace(0,1,M), a[i], kind='linear')\n",
    "        a_interp[i] = f(np.linspace(0, 1, 40))\n",
    "\n",
    "    return a_interp\n",
    "\n",
    "def processTime(t, yy, ddn, lat, lon, ABI_ROOT):\n",
    "    if np.floor(t) < 19:\n",
    "        raise ValueError(\"Times must be between 19-23\")\n",
    "\n",
    "    if lat < latMin or lat > latMax or lon < longMin or lon > longMax:\n",
    "        raise ValueError(\"Latitude and Longitude are not correctly bounded\")\n",
    "    # # Find the proper index using Manhattan distance\n",
    "    # distances = np.abs(abiLat - lat) + np.abs(abiLong - lon)\n",
    "\n",
    "    # # distances = distances[BOUND_SIZE:LENGTH-BOUND_SIZE,BOUND_SIZE:LENGTH-BOUND_SIZE]\n",
    "    \n",
    "    # # Find minimum value in distances array\n",
    "    # coords = np.unravel_index(distances.argmin(), distances.shape)\n",
    "    # # coords = np.array(coords) + BOUND_SIZE\n",
    "    # # if coords[0] < BOUND_SIZE or coords[1] < BOUND_SIZE or coords[1] > LENGTH-BOUND_SIZE or coords[0] > LENGTH-BOUND_SIZE:\n",
    "    # #     raise ImportError(\"Bad latitude and longitude\")\n",
    "    # MORE OPTIMIZED: Find the proper index using Binary Search\n",
    "\n",
    "    AREA_SIZE = 1000\n",
    "\n",
    "    lati = len(latSlice) - np.searchsorted(latSlice, lat) + 17\n",
    "    loni = np.searchsorted(longSlice, lon) + 18\n",
    "    \n",
    "    distances = np.abs(abiLat[lati-AREA_SIZE:lati+AREA_SIZE, loni-AREA_SIZE:loni+AREA_SIZE] - lat) + np.abs(abiLong[lati-AREA_SIZE:lati+AREA_SIZE, loni-AREA_SIZE:loni+AREA_SIZE] - lon)\n",
    "    coords = np.array(np.unravel_index(distances.argmin(), distances.shape))\n",
    "    if coords[0] == 0 or coords[1] == 0 or coords[1] == 2*AREA_SIZE-1 or coords[0] == 2*AREA_SIZE - 1:\n",
    "        print(\"FALLBACK\")\n",
    "        distances = np.abs(abiLat - lat) + np.abs(abiLong - lon)\n",
    "        coords = np.unravel_index(distances.argmin(), distances.shape)\n",
    "    else:\n",
    "        coords[0] += lati - AREA_SIZE\n",
    "        coords[1] += loni - AREA_SIZE\n",
    "\n",
    "    if coords[0] < BOUND_SIZE or coords[1] < BOUND_SIZE or coords[1] > LENGTH-BOUND_SIZE or coords[0] > LENGTH-BOUND_SIZE:\n",
    "        raise ValueError(\"Bad latitude and longitude\")\n",
    "        \n",
    "    # print(coords[0] - ncoords[0], coords[1] - ncoords[1])\n",
    "\n",
    "    hour = np.floor(t).astype(int)\n",
    "\n",
    "    # Gather the ABI_FILE\n",
    "    DATA = GATHERED_ABI_FILES.get(f'{yy}-{ddn}-{hour}')\n",
    "    if DATA is None:\n",
    "        DATA = gather_files(str(yy), str(ddn), str(hour), ABI_ROOT)\n",
    "        GATHERED_ABI_FILES[f'{yy}-{ddn}-{hour}'] = DATA\n",
    "\n",
    "    # Find the closest minute\n",
    "    if DATA[\"everyten\"]:\n",
    "        minutes = np.round((t - np.floor(t)) * 6).astype(int) * 10\n",
    "    else:\n",
    "        minutes = np.round((t - np.floor(t)) * 4).astype(int) * 15\n",
    "\n",
    "    # If minutes is closer to the next hour, shift the hour forward by one, unless that would cause a day shift\n",
    "    if minutes == 60:\n",
    "        if hour != 23:\n",
    "            hour += 1\n",
    "            minutes = 0\n",
    "        else:\n",
    "            if DATA[\"everyten\"]:\n",
    "                minutes = 50\n",
    "            else:\n",
    "                minutes = 45\n",
    "\n",
    "    minutes = str(minutes)\n",
    "    if minutes == \"0\":\n",
    "        minutes =\"00\"\n",
    "\n",
    "    ABI = COLLECTED_ABI_DATA.get(f'{yy}-{ddn}-{hour}-{minutes}')\n",
    "    if ABI is None:\n",
    "        ABI = get_L1B_L2(DATA[minutes], DATA[\"L200\"], DATA[\"YYYY\"], DATA[\"DDD\"], DATA[\"HH\"], ABI_ROOT)\n",
    "        COLLECTED_ABI_DATA[f'{yy}-{ddn}-{hour}-{minutes}'] = ABI\n",
    "\n",
    "    chip = ABI[coords[0]-64:coords[0]+64, coords[1]-64:coords[1]+64, :]\n",
    "\n",
    "    return chip, coords\n",
    "\n",
    "translation = [1, 2, 0, 4, 5, 6, 3, 8, 9, 10, 11, 13, 14, 15]\n",
    "    \n",
    "def processFile(yy, ddn, orbit, latb):\n",
    "    cloudsatpath = '/explore/nobackup/people/jgong/cloudsat/'\n",
    "    #cloudsatpath = '/discover/nobackup/jgong/cloudsat/'\n",
    "    cs_file = glob.glob(cloudsatpath+'2B-CLDCLASS-LIDAR/'+yy+'/'+ddn+'/'+yy+ddn+'*'+orbit+'*2B-CLDCLASS-LIDAR*'+'P1_R05*.hdf')\n",
    "    aux_file = glob.glob(cloudsatpath+'ECMWF-AUX/'+yy+'/'+ddn+'/'+yy+ddn+'*'+orbit+'*ECMWF-AUX*'+'P1_R05*.hdf')\n",
    "\n",
    "    if len(cs_file) == 0 or len(aux_file) == 0:\n",
    "        print(\"NO CS OR AUX\")\n",
    "        return\n",
    "\n",
    "    [Pressure,DEM_elevation,Temperature,Specific_humidity,EC_height,Temperature_2m,\\\n",
    "        U10_velocity,V10_velocity,UTC_Time] = read_cs_ecmwf(aux_file[0],latbin=latb)\n",
    "\n",
    "    ##========== read 2b-cldclass-lidar ================================\n",
    "    [cs_clb,cs_clt,cs_QC,Latitude,Longitude] = read_2b_cldclass_lidar(cs_file[0],latbin=latb)\n",
    "    \n",
    "    Pressure = interpArray(Pressure)\n",
    "    Temperature = interpArray(Temperature)\n",
    "    Specific_humidity = interpArray(Specific_humidity)\n",
    "    \n",
    "    N = len(cs_clb)\n",
    "\n",
    "    cs_clb_2 = np.floor(2 * cs_clb).astype(int)\n",
    "    cs_clt_2 = np.floor(2 * cs_clt).astype(int)\n",
    "    \n",
    "    newRescaled = np.zeros((N, 40)).astype(int)\n",
    "    for i in range(N):\n",
    "        for j in range(10):\n",
    "            if cs_clb[i, j] == -99:\n",
    "                break\n",
    "            newRescaled[i, cs_clb_2[i, j]:cs_clt_2[i, j]+1] = 1\n",
    "\n",
    "    # Find corresponding ABI file to the UTC_Time\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while i < N:\n",
    "        try:\n",
    "            chip, coords = processTime(UTC_Time[i], yy, ddn, Latitude[i], Longitude[i], '/explore/nobackup/people/szhang16/ABI-L1b-RadF/')\n",
    "        except ValueError:\n",
    "            i += 20\n",
    "            continue\n",
    "        except ImportError:\n",
    "            print(i)\n",
    "            i += 90\n",
    "            continue\n",
    "            \n",
    "        # dRange = np.arange(i-45, i+46)\n",
    "        # Reverse the CloudSAT Data\n",
    "        dRange = np.arange(i+46, i-45, -1)\n",
    "        aux_data = {\n",
    "            \"Pressure\": Pressure[dRange],\n",
    "            \"DEM_elevation\": DEM_elevation[dRange],\n",
    "            \"Temperature\": Temperature[dRange],\n",
    "            \"Specific_humidity\": Specific_humidity[dRange],\n",
    "            \"Temperature_2m\": Temperature_2m[dRange],\n",
    "            \"U10_velocity\": U10_velocity[dRange],\n",
    "            \"V10_velocity\": V10_velocity[dRange],\n",
    "            \"UTC_Time\": UTC_Time[dRange],\n",
    "            \"Latitude\": Latitude[dRange],\n",
    "            \"Longitude\": Longitude[dRange],\n",
    "            \"Cloud_mask\": newRescaled[dRange],\n",
    "        }\n",
    "\n",
    "        fileName = f'{yy}-{ddn}-{orbit}_{coords[0]}-{coords[1]}'\n",
    "\n",
    "        chip = chip[..., translation]\n",
    "\n",
    "        np.savez('/explore/nobackup/people/szhang16/abiCloudSATChips/' + fileName, chip=chip, data=aux_data)\n",
    "        print(fileName)\n",
    "\n",
    "        i += 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca22c8-1867-4a20-9373-2ad26c4c5dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019285194846_71688_CS_2B-CLDCLASS-LIDAR_GRANULE_P1_R05_E09_F00.hdf\n",
      "3860\n",
      "3950\n",
      "4040\n",
      "4130\n",
      "4220\n",
      "4310\n",
      "4400\n",
      "4490\n",
      "4580\n",
      "4670\n",
      "4760\n",
      "4850\n",
      "4940\n",
      "5030\n",
      "5120\n",
      "5210\n",
      "5300\n",
      "5390\n",
      "5480\n",
      "5570\n",
      "5660\n",
      "5750\n",
      "5840\n",
      "5930\n",
      "6020\n",
      "6110\n",
      "6200\n",
      "6290\n",
      "6380\n",
      "6470\n",
      "6560\n",
      "6650\n",
      "6740\n",
      "6830\n",
      "6920\n",
      "7010\n",
      "7100\n",
      "7190\n",
      "7280\n",
      "7370\n",
      "7460\n",
      "7550\n",
      "7640\n",
      "7730\n",
      "7820\n",
      "7910\n",
      "8000\n",
      "8090\n",
      "8180\n",
      "8270\n",
      "8360\n",
      "8450\n",
      "8540\n",
      "8630\n",
      "8720\n",
      "8810\n",
      "8900\n",
      "8990\n",
      "9080\n",
      "9170\n",
      "9260\n",
      "9350\n",
      "9440\n",
      "9530\n",
      "9620\n",
      "9710\n",
      "9800\n",
      "9890\n",
      "9980\n",
      "10070\n",
      "10160\n",
      "10250\n",
      "10340\n",
      "10430\n",
      "10520\n",
      "10610\n",
      "10700\n",
      "10790\n",
      "10880\n",
      "10970\n",
      "2019285230550_71690_CS_2B-CLDCLASS-LIDAR_GRANULE_P1_R05_E09_F00.hdf\n",
      "2019285212718_71689_CS_2B-CLDCLASS-LIDAR_GRANULE_P1_R05_E09_F00.hdf\n"
     ]
    }
   ],
   "source": [
    "#=================================\n",
    "#    MAIN\n",
    "#=================================\n",
    "# yy = '2019'\n",
    "# ddn = '045'; orbit = '68181'\n",
    "\n",
    "# processFile(yy, ddn, orbit, (latMin, latMax))\n",
    "\n",
    "for filename in os.listdir('/explore/nobackup/people/jgong/cloudsat/2B-CLDCLASS-LIDAR/2019/285'):\n",
    "    if filename.endswith('.hdf'):\n",
    "        if int(filename[7:9]) > 18:\n",
    "            print(filename)\n",
    "            processFile(\"2019\", \"285\", filename[14:19], (latMin, latMax))\n",
    "\n",
    "# ROOT_DIR = '/explore/nobackup/people/jgong/cloudsat/2B-CLDCLASS-LIDAR'\n",
    "\n",
    "# for year in os.listdir(ROOT_DIR):\n",
    "#     if year == \"2018\":\n",
    "#         continue\n",
    "#     for day in os.listdir(ROOT_DIR + '/' + year):\n",
    "#         for file in os.listdir(ROOT_DIR + '/' + year + '/' + day):\n",
    "#             if file.endswith('hdf'):\n",
    "#                 orbit = file[14:19]\n",
    "#                 if int(file[7:9]) > 18:\n",
    "#                     print(f\"PROCESSING FILE {year} {day} {orbit}\")\n",
    "#                     processFile(year, day, orbit, (latMin, latMax))\n",
    "\n",
    "# for i in np.arange(10):\n",
    "#   tmp=np.squeeze(cs_clt[:,i])\n",
    "#   ind=np.argwhere(tmp>0)\n",
    "#   print(len(ind),len(tmp))\n",
    "\n",
    "#---- implementing the 40-level cloud mask processing code from ----\n",
    "#---- https://colab.research.google.com/drive/1cIsSLS5ht8PXQ8bAbJpboAIxrnanwupu -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2526f-f500-41f5-9bbe-9e02f17f955e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab-pytorch]",
   "language": "python",
   "name": "conda-env-ilab-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
