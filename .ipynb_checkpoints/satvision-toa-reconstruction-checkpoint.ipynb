{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ab2075-c488-46b9-8cd2-0cdaf399acfc",
   "metadata": {},
   "source": [
    "# Satvision-TOA Reconstruction Notebook\n",
    "\n",
    "Version: 04.30.24\n",
    "\n",
    "Env: `Python [conda env:ilab-pytorch]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88ea70-7dbf-4b67-a12d-db36e2bc9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install yacs timm segmentation-models-pytorch termcolor webdataset==0.2.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046c3e5-c458-4e03-9c96-e9eb95a04963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import netCDF4 as nc\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7db1bc-09ee-47e3-9015-e6b148d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./pytorch-caney')\n",
    "\n",
    "from pytorch_caney.config import get_config\n",
    "\n",
    "from pytorch_caney.models.build import build_model\n",
    "\n",
    "from pytorch_caney.ptc_logging import create_logger\n",
    "\n",
    "from pytorch_caney.data.datasets.mim_modis_22m_dataset import MODIS22MDataset\n",
    "\n",
    "from pytorch_caney.data.transforms import SimmimTransform, SimmimMaskGenerator\n",
    "\n",
    "from pytorch_caney.config import _C, _update_config_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841e464-f880-4e53-bf31-f9f225713918",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274e323-bc04-41d4-bc49-baed65d027e6",
   "metadata": {},
   "source": [
    "### Clone model ckpt from huggingface\n",
    "\n",
    "```bash\n",
    "# On prism/explore\n",
    "module load git-lfs\n",
    "\n",
    "git lfs install\n",
    "\n",
    "git clone git clone git@hf.co:nasa-cisto-data-science-group/satvision-toa-huge-patch8-window12-192\n",
    "```\n",
    "\n",
    "Note: If using git w/ ssh, make sure you have ssh keys enabled to clone using ssh auth.\n",
    "https://huggingface.co/docs/hub/security-git-ssh\n",
    "\n",
    "```bash\n",
    "eval $(ssh-agent)\n",
    "\n",
    "# If this outputs as anon, follow the next steps.\n",
    "ssh -T git@hf.co\n",
    "\n",
    "# Check if ssh-agent is using the proper key\n",
    "ssh-add -l\n",
    "\n",
    "# If not\n",
    "ssh-add ~/.ssh/your-key\n",
    "\n",
    "# Or if you want to use the default id_* key, just do\n",
    "ssh-add\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af699ba3-2d98-4daf-9437-c322d7b59a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH: str = '/explore/nobackup/people/szhang16/satvision-toa-huge-patch8-window8-128/mp_rank_00_model_states.pt'\n",
    "CONFIG_PATH: str = '/explore/nobackup/people/szhang16/satvision-toa-huge-patch8-window8-128/mim_pretrain_swinv2_satvision_huge_128_window8_patch8_100ep.yaml'\n",
    "\n",
    "OUTPUT: str = '.'\n",
    "TAG: str = 'satvision-huge-toa-reconstruction'\n",
    "DATA_PATH: str = '/home/szhang16/modis-toa-samples_03_31/selected_satvision_toa_2m_128_chips.npy'\n",
    "DATA_PATHS: list = [DATA_PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4593e8c-6e94-4d01-b86e-5b78b621fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config given configurations\n",
    "\n",
    "config = _C.clone()\n",
    "_update_config_from_file(config, CONFIG_PATH)\n",
    "\n",
    "config.defrost()\n",
    "config.MODEL.RESUME = MODEL_PATH\n",
    "config.DATA.DATA_PATHS = DATA_PATHS\n",
    "config.OUTPUT = OUTPUT\n",
    "config.TAG = TAG\n",
    "config.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a4474-88e4-44d5-b899-7aaf6cbed6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='app.log',  # Specify the log file name\n",
    "    level=logging.INFO,  # Set logging level to DEBUG\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',  # Specify log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'  # Specify date format\n",
    ")\n",
    "\n",
    "# Add logging to standard output\n",
    "console = logging.StreamHandler()  # Create a handler for standard output\n",
    "console.setLevel(logging.INFO)  # Set logging level for standard output\n",
    "console.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))  # Set log message format for standard output\n",
    "logger = logging.getLogger('')\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebd497-7741-41a7-af9d-0ee49a6313a4",
   "metadata": {},
   "source": [
    "## 2. Load model weights from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abf348-c6bf-43a3-b00a-cc5f8d80545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_PATH)\n",
    "model = build_model(config, pretrain=True)\n",
    "model.load_state_dict(checkpoint['module']) # If 'module' not working, try 'model'\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"number of params: {n_parameters}\")\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b500d13b-89d7-4cd8-a36a-ab6f10f6a397",
   "metadata": {},
   "source": [
    "## 3. Load evaluation set (from numpy file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a455f9-1f90-48a1-966b-bb334d683c21",
   "metadata": {
    "id": "7a1f48d1-e6ee-4f14-a471-a292368b6db9"
   },
   "outputs": [],
   "source": [
    "def find_files(directory, prefix):\n",
    "    \"\"\"\n",
    "    Find files in a directory that start with a specified prefix.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory path to search for files.\n",
    "        prefix (str): The prefix that the filenames should start with.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths matching the specified prefix.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(prefix):\n",
    "            matching_files.append(os.path.join(directory, filename))\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da5cc8-1dc2-45e3-950d-9b6d6a97d11d",
   "metadata": {
    "id": "3882d9d3-942f-4417-b9c0-57392e3e192f"
   },
   "outputs": [],
   "source": [
    "def gather_files(YYYY, DDD, HH):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        YYYY (str): year in YYYY format\n",
    "        DDD (str): day in DDD format\n",
    "        HH (str): hour in HH format\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing â€”\n",
    "                ROOT_PATH: the base abi path\n",
    "\n",
    "                YYYY: year\n",
    "                DDD: day of year\n",
    "                HH: hour\n",
    "\n",
    "                00: files for minute 00 in hour\n",
    "                ...\n",
    "                50: filea for minute 50 in hour\n",
    "\n",
    "                L200: cloud top height file (full path) for minute 00 in hour\n",
    "                ...\n",
    "                L250: cloud top height file (full path) for minute 50 in hour\n",
    "    \"\"\"\n",
    "\n",
    "    ABI_ = {\n",
    "        \"ROOT_PATH\": None,\n",
    "\n",
    "        \"YYYY\": None,\n",
    "        \"DDD\": None,\n",
    "        \"HH\": None,\n",
    "\n",
    "        \"00\": [],\n",
    "        \"15\": [],\n",
    "        \"30\": [],\n",
    "        \"45\": [],\n",
    "\n",
    "        \"L200\": None,\n",
    "        \"L210\": None,\n",
    "        \"L220\": None,\n",
    "        \"L230\": None,\n",
    "        \"L240\": None,\n",
    "        \"L250\": None\n",
    "    }\n",
    "\n",
    "    _ABI_PATH_ = \"/css/geostationary/BackStage/GOES-16-ABI-L1B-FULLD/\" + YYYY + \"/\" + DDD + \"/\" + HH\n",
    "\n",
    "    for filename in os.listdir(_ABI_PATH_):\n",
    "        if ABI_[\"ROOT_PATH\"] == None:\n",
    "            ABI_[\"ROOT_PATH\"] = _ABI_PATH_\n",
    "            ABI_[\"YYYY\"] = filename[27:31]\n",
    "            ABI_[\"DDD\"] = filename[31:34]\n",
    "            ABI_[\"HH\"] = filename[34:36]\n",
    "        MM = filename[36:38]\n",
    "        ABI_[f\"{MM}\"].append(filename)\n",
    "\n",
    "    return ABI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80f132-07b8-496f-a84b-a5d35e68d3d0",
   "metadata": {
    "id": "e7427194-7847-4e51-b24f-efb97a50cb14"
   },
   "outputs": [],
   "source": [
    "def get_L1B_L2(abipaths, l2path, YYYY, DDD, HH):\n",
    "\n",
    "    # ASSERT ALL ABI CHANNELS PRESENT !!!!!!!!!!!!!!!!!!!!\n",
    "    # assert len(abipaths) == 8, \"NOT ALL ABI FILES LOCATED\"\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "    # LOAD EACH ABI CHANNEL IMAGE\n",
    "    CHANNELS = []\n",
    "    ROOT = \"/css/geostationary/BackStage/GOES-16-ABI-L1B-FULLD/\"\n",
    "\n",
    "    for file in abipaths:\n",
    "        L1B = np.array(nc.Dataset(ROOT + \"/\" + YYYY + \"/\" + DDD + \"/\" + HH + \"/\" + file, 'r')[\"Rad\"])\n",
    "        CHANNEL = int(file[19:21])\n",
    "        CHANNELS.append((L1B, CHANNEL))\n",
    "\n",
    "    # SORT CHANNELS\n",
    "    CHANNELS.sort(key=lambda x: x[1])\n",
    "    CHANNELS = [C[0] for C in CHANNELS]\n",
    "\n",
    "    T = []\n",
    "    #RESIZE ALL CHANNELS TO SAME SIZE\n",
    "    for C in CHANNELS:\n",
    "        S = C.shape[0] // 5424\n",
    "        if S != 1: C = C[::S, ::S]\n",
    "        T.append(C)\n",
    "\n",
    "    CHANNELS = T\n",
    "\n",
    "    # STACK ABI CHANNELS INTO SINGLE IMAGE\n",
    "    ABI = np.stack(CHANNELS, axis=2)\n",
    "\n",
    "    return ABI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b717e-d427-4fa8-9c9d-208709129b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = gather_files(\"2017\", \"218\", \"18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116eba0-8581-4629-86dc-2d541522f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABI = get_L1B_L2(DATA[\"00\"], DATA[\"L200\"], DATA[\"YYYY\"], DATA[\"DDD\"], DATA[\"HH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f50259-8d2c-4593-b542-d3ee4bb657d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ABI[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e958ad2-bb73-4037-b87c-71faea548df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2 * N random locations within the Bounds\n",
    "\n",
    "BOUNDS = (1000, 5424-1000)\n",
    "N = 100\n",
    "\n",
    "# GENERATE 2*N COORDINATES AND ENSURE BALANCED DISTRIBUTION\n",
    "Xs = np.random.uniform(BOUNDS[0], BOUNDS[1], int(2 * N)).astype(int)\n",
    "Ys = np.random.uniform(BOUNDS[0], BOUNDS[1], int(2 * N)).astype(int)\n",
    "coordinate_pairs = list(zip(Xs, Ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78c4ce-24b2-43f1-8997-b0d1dbce8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vis_calibrate(data):\n",
    "    \"\"\"Calibrate visible channels to reflectance.\"\"\"\n",
    "    solar_irradiance = np.array(2017)\n",
    "    esd = np.array(0.99)\n",
    "    factor = np.pi * esd * esd / solar_irradiance\n",
    "\n",
    "    return data * np.float32(factor) * 100\n",
    " \n",
    "def _ir_calibrate(data):\n",
    "    \"\"\"Calibrate IR channels to BT.\"\"\"\n",
    "    fk1 = np.array(13432.1),\n",
    "    fk2 = np.array(1497.61),\n",
    "    bc1 = np.array(0.09102),\n",
    "    bc2 = np.array(0.99971),\n",
    "\n",
    "    # if self.clip_negative_radiances:\n",
    "    #     min_rad = self._get_minimum_radiance(data)\n",
    "    #     data = data.clip(min=data.dtype.type(min_rad))\n",
    "\n",
    "    res = (fk2 / np.log(fk1 / data + 1) - bc1) / bc2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ca4da-ae55-4ce2-b4c0-cc9967c87758",
   "metadata": {},
   "outputs": [],
   "source": [
    "abiData = []\n",
    "\n",
    "translation = [1, 2, 0, 4, 5, 6, 3, 8, 9, 10, 11, 13, 14, 15]\n",
    "mask = np.array([True, True, True, True, True, False, True, False, False, False, False, False, False, False])\n",
    "\n",
    "# Maximum amount of chips\n",
    "MAXCHIPS = 100\n",
    "\n",
    "for (x, y) in coordinate_pairs:\n",
    "    chip = ABI[x-64:x+64, y-64:y+64, :]\n",
    "    \n",
    "    if MAXCHIPS < 1:\n",
    "        break\n",
    "\n",
    "    skip_chip = False\n",
    "\n",
    "    # If there's any non values, which are always the next highest power of 2 - 1, skip the chip\n",
    "    \n",
    "    for exp in range(10, 16):\n",
    "        if np.isin(2**exp-1, chip):\n",
    "            skip_chip = True\n",
    "\n",
    "    if skip_chip:\n",
    "        continue\n",
    "\n",
    "    MAXCHIPS -= 1\n",
    "\n",
    "    # Reorder the channels\n",
    "    \n",
    "    chip = chip[..., translation]\n",
    "\n",
    "    # Convert from irradiance to respective units\n",
    "    \n",
    "    chip[..., mask] = _vis_calibrate(chip[..., mask])\n",
    "    chip[..., ~mask] = _ir_calibrate(chip[..., ~mask])\n",
    "\n",
    "    abiData.append(chip)\n",
    "\n",
    "abiData = np.array(abiData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6d99a-7ec1-4bfc-aa1a-20b059cd065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abiData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361dbc3-723d-4845-97cc-edd0743ea312",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/explore/nobackup/people/szhang16/2017-218-18-abichips-100.npy', 'wb') as f:\n",
    "    np.save(f, abiData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8d307-de9b-4617-abdd-dae1e7c2521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Masked-Image-Modeling transform\n",
    "transform = SimmimTransform(config)\n",
    "\n",
    "# The reconstruction evaluation set is a single numpy file\n",
    "# validation_dataset_path = config.DATA.DATA_PATHS[0]\n",
    "# validation_dataset = np.load(validation_dataset_path)\n",
    "# print(validation_dataset.shape)\n",
    "\n",
    "# Load in the abiData\n",
    "validation_dataset = abiData\n",
    "# validation_dataset = origChips\n",
    "\n",
    "# validation_dataset = validation_dataset[:5]\n",
    "len_batch = range(validation_dataset.shape[0])\n",
    "\n",
    "# Apply transform to each image in the batch\n",
    "# A mask is auto-generated in the transform\n",
    "imgMasks = [transform(validation_dataset[idx]) for idx \\\n",
    "    in len_batch]\n",
    "\n",
    "# Seperate img and masks, cast masks to torch tensor\n",
    "img = torch.stack([imgMask[0] for imgMask in imgMasks])\n",
    "mask = torch.stack([torch.from_numpy(imgMask[1]) for \\\n",
    "    imgMask in imgMasks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55acf5e9-eb2a-496c-baa6-3b74503a2978",
   "metadata": {},
   "source": [
    "## 4. Prediction helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595336f8-71b4-418b-b153-2461583ed613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, num_batches=5):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    masks = []\n",
    "    losses = []\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "\n",
    "        for idx, img_mask in enumerate(dataloader):\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "            if idx > num_batches:\n",
    "                return inputs, outputs, masks, losses\n",
    "\n",
    "            img_mask = img_mask[0]\n",
    "\n",
    "            img = torch.stack([pair[0] for pair in img_mask])\n",
    "            mask = torch.stack([pair[1] for pair in img_mask])\n",
    "\n",
    "            img = img.cuda(non_blocking=True)\n",
    "            mask = mask.cuda(non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with amp.autocast(enabled=config.ENABLE_AMP):\n",
    "                    z = model.encoder(img, mask)\n",
    "                    img_recon = model.decoder(z)\n",
    "                    loss = model(img, mask)\n",
    "\n",
    "            inputs.extend(img.cpu())\n",
    "            masks.extend(mask.cpu())\n",
    "            outputs.extend(img_recon.cpu())\n",
    "            losses.append(loss.cpu())\n",
    "    \n",
    "    return inputs, outputs, masks, losses\n",
    "\n",
    "\n",
    "def minmax_norm(img_arr):\n",
    "    arr_min = img_arr.min()\n",
    "    arr_max = img_arr.max()\n",
    "    img_arr_scaled = (img_arr - arr_min) / (arr_max - arr_min)\n",
    "    img_arr_scaled = img_arr_scaled * 255\n",
    "    img_arr_scaled = img_arr_scaled.astype(np.uint8)\n",
    "    return img_arr_scaled\n",
    "\n",
    "\n",
    "def process_mask(mask):\n",
    "    mask_img = mask.unsqueeze(0)\n",
    "    mask_img = mask_img.repeat_interleave(4, 1).repeat_interleave(4, 2).unsqueeze(1).contiguous()\n",
    "    mask_img = mask_img[0, 0, :, :]\n",
    "    mask_img = np.stack([mask_img, mask_img, mask_img], axis=-1)\n",
    "    return mask_img\n",
    "\n",
    "\n",
    "def process_prediction(image, img_recon, mask, rgb_index):\n",
    "\n",
    "    mask = process_mask(mask)\n",
    "    \n",
    "    red_idx = rgb_index[0]\n",
    "    blue_idx = rgb_index[1]\n",
    "    green_idx = rgb_index[2]\n",
    "\n",
    "    image = image.numpy()\n",
    "    rgb_image = np.stack((image[red_idx, :, :],\n",
    "                          image[blue_idx, :, :],\n",
    "                          image[green_idx, :, :]),\n",
    "                         axis=-1)\n",
    "    rgb_image = minmax_norm(rgb_image)\n",
    "\n",
    "    img_recon = img_recon.numpy()\n",
    "    \n",
    "#     idx = 1\n",
    "    \n",
    "#     for channel in img_recon:\n",
    "#         print(f'Channel #{idx}')\n",
    "#         idx += 1\n",
    "#         print(\"MIN: \", end=\"\")\n",
    "#         print(channel.min())\n",
    "#         print(\"MAX: \", end=\"\")\n",
    "#         print(channel.max())\n",
    "    \n",
    "    rgb_image_recon = np.stack((img_recon[red_idx, :, :],\n",
    "                                img_recon[blue_idx, :, :],\n",
    "                                img_recon[green_idx, :, :]),\n",
    "                                axis=-1)\n",
    "    rgb_image_recon = minmax_norm(rgb_image_recon)\n",
    "\n",
    "    rgb_masked = np.where(mask == 0, rgb_image, rgb_image_recon)\n",
    "    rgb_image_masked = np.where(mask == 1, 0, rgb_image)\n",
    "    rgb_recon_masked = rgb_masked\n",
    "    \n",
    "    return rgb_image, rgb_image_masked, rgb_recon_masked, mask\n",
    "\n",
    "olosses = []\n",
    "\n",
    "for i in range(14):\n",
    "    olosses.append([])\n",
    "\n",
    "lossMSE = torch.nn.MSELoss()\n",
    "\n",
    "def plot_export_pdf(path, inputs, outputs, masks, rgb_index):\n",
    "    pdf_plot_obj = PdfPages(path)\n",
    "\n",
    "    # Loop through each chip\n",
    "    for idx in range(len(inputs)):\n",
    "        # prediction processing\n",
    "        image = inputs[idx]\n",
    "        img_recon = outputs[idx]\n",
    "        mask = masks[idx]\n",
    "\n",
    "        # Get the MSE loss per channel\n",
    "\n",
    "        for i in range(14):\n",
    "            oloss = lossMSE(image[i, ...], img_recon[i, ...])\n",
    "            olosses[i].append(oloss.item())\n",
    "\n",
    "    # CODE FOR PLOTTING ALL 14 CHANNELS AND THEIR RECONSTRUCTIONS\n",
    "    \n",
    "    #     rgb_image, rgb_image_masked, rgb_recon_masked, mask = \\\n",
    "    #         process_prediction(image, img_recon, mask, rgb_index)\n",
    "\n",
    "    #     # matplotlib code\n",
    "    #     # Loop through each channel\n",
    "    #     fig, axs = plt.subplots(14, 2, figsize=(15, 105))\n",
    "    #     for i in range(14):\n",
    "    #         im0 = axs[i][0].imshow(img_recon[i, :, :])\n",
    "    #         im1 = axs[i][1].imshow(image[i, :, :])\n",
    "    #         plt.colorbar(im0)\n",
    "    #         plt.colorbar(im1)\n",
    "    #         axs[i][0].set_title(f\"Image {idx} Channel Index {i} Model Reconstruction\")\n",
    "    #         axs[i][1].set_title(f\"Image {idx} Channel Index {i} Original\")\n",
    "    #     plt.show()\n",
    "    #     pdf_plot_obj.savefig(fig)\n",
    "\n",
    "    # pdf_plot_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c44b5-6d88-45c4-b397-c38de8064544",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e695cc3-b869-4fc2-b360-b45f3b81affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "masks = []\n",
    "losses = []\n",
    "\n",
    "# We could do this in a single batch however we\n",
    "# want to report the loss per-image, in place of\n",
    "# loss per-batch.\n",
    "for i in tqdm(range(img.shape[0])):\n",
    "    single_img = img[i].unsqueeze(0)\n",
    "    single_mask = mask[i].unsqueeze(0)\n",
    "    single_img = single_img.cuda(non_blocking=True)\n",
    "    single_mask = single_mask.cuda(non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encoder(single_img, single_mask)\n",
    "        img_recon = model.decoder(z)\n",
    "        loss = model(single_img, single_mask)\n",
    "\n",
    "    inputs.extend(single_img.cpu())\n",
    "    masks.extend(single_mask.cpu())\n",
    "    outputs.extend(img_recon.cpu())\n",
    "    losses.append(loss.cpu()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f102c-94df-4d9e-8040-52197a7e71db",
   "metadata": {},
   "source": [
    "## 6. Plot and write to PDF\n",
    "\n",
    "Writes out all of the predictions to a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdcd1d-09db-4ccf-8cc1-58d6f47e3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './selected_satvision-toa-reconstruction-14-channels.pdf'\n",
    "rgb_index = [0, 2, 1] # Indices of [Red band, Blue band, Green band]\n",
    "\n",
    "plot_export_pdf(pdf_path, inputs, outputs, masks, rgb_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a57f4d-5df0-47a3-bfb6-d7f29a95e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Losses\")\n",
    "print(len(losses))\n",
    "print(np.mean(losses))\n",
    "print(np.std(losses))\n",
    "print(np.min(losses))\n",
    "print(np.max(losses))\n",
    "\n",
    "olosses = np.array(olosses)\n",
    "print(\"MSE\")\n",
    "for i in range(14):\n",
    "    print(f\"Channel Idx: {i}\")\n",
    "    print(f\"{np.mean(olosses[i])}\")\n",
    "    print(f\"{np.std(olosses[i])}\")\n",
    "    print(f\"{np.min(olosses[i])}\")\n",
    "    print(f\"{np.max(olosses[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73745d-21d0-4a94-90a1-e4544f43a4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab-pytorch]",
   "language": "python",
   "name": "conda-env-ilab-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
